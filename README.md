# ðŸ§  Generative Text Model using GPT-2

This project demonstrates a generative text model using the pre-trained GPT-2 model from HuggingFace. It takes a user-defined topic or prompt and generates a coherent paragraph using the power of transformer-based language models.

---

## ðŸ“Œ Features

- Generate coherent paragraphs from any user-given prompt
- Uses pre-trained GPT-2 model (no training required)
- Easy to run on Google Colab
- Code is beginner-friendly and modular

---

## ðŸš€ How to Run

### 1. Open in Google Colab
Click below to open the notebook:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rohan-5076/generative-text-model/blob/main/text_generation_gpt.ipynb)

### 2. Run the Cells
- Install dependencies (`transformers`)
- Enter your prompt when asked
- Generate paragraph with GPT-2

---

## ðŸ§ª Example

**Prompt:**
